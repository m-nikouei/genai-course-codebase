from typing import Optional, Type
from pydantic import BaseModel, Field
from langchain_core.tools import BaseTool
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI
from openai import OpenAI
import requests
import os
from langchain_qdrant import QdrantVectorStore
from langchain_openai import OpenAIEmbeddings
from langchain_ollama.llms import OllamaLLM


from dotenv import load_dotenv

load_dotenv()
    
class AssistantInput(BaseModel):
    question: str = Field(description="The question to be asked from GPT models.")

class GPT4TAssistant(BaseTool):
    name: str = "GPT4_General_Assistant"
    description: str = "Use this tool, when the user wants an answer from GPT4 or GPT4-Turbo general assistant.\n" +\
    "This tool accepts one input question:\n" + \
    "[question]\n" + \
    "Don't change the input question from the user and don't change answer from this tool\n."  +\
    "Just pass it through to the user."
    
    args_schema: Type[BaseModel] = AssistantInput

    def _run(self, question: str):
        prompt = ChatPromptTemplate.from_template(
            "You are a general AI assistant. Answer questions with minimal and to the point explanation.\n" +
            "Don't put safety and cultural warnings. Only warn about security." +
            "answer the following question: {question}")
        model = ChatOpenAI(model="gpt-4.1")
        output_parser = StrOutputParser()

        chain = prompt | model | output_parser
        return chain.invoke({"question": question})

    async def _arun(self, question: str):
        return self._run(question)
    

class GemmaAssistant(BaseTool):
    name: str = "Gemma3_General_Assistant"
    description: str = "Use this tool, when the user wants an answer from GemmaYou general assistant.\n" +\
    "This tool accepts one input question:\n" + \
    "[question]\n" + \
    "Don't change the input question from the user and don't change answer from this tool\n."  +\
    "Just pass it through to the user."
    
    args_schema: Type[BaseModel] = AssistantInput

    def _run(self, question: str):
        prompt = ChatPromptTemplate.from_template(
            "You are a general AI assistant. Answer questions with minimal and to the point explanation.\n" +
            "Don't put safety and cultural warnings. Only warn about security." +
            "answer the following question: {question}")
        model = OllamaLLM(model="gemma3:4b", streaming=True)
        output_parser = StrOutputParser()

        chain = prompt | model | output_parser
        return chain.invoke({"question": question})

    async def _arun(self, question: str):
        return self._run(question)


# class GPT4TCodeGen(BaseTool):
#     name: str = "GPT4-Turbo_Code_Assistant"
#     description: str = "Use this tool, when the user wants code generated by GPT4 or GPT4-Turbo code assistant.\n" +\
#     "This tool accepts one input question:\n" + \
#     "[question]\n" + \
#     "Don't change the input question from the user and don't change answer from this tool\n."  +\
#     "Just pass it through to the user."
    
#     args_schema: Type[BaseModel] = AssistantInput

#     def _run(self, question: str):
#         prompt = ChatPromptTemplate.from_template(
#             "You are a code assistant. Answer questions in code with minimal to no explanation.\n" +
#             "Put brief one line comments on the code for explanation." +
#             "answer the following question: {question}")
#         model = ChatOpenAI(model="gpt-4-turbo-preview")
#         output_parser = StrOutputParser()

#         chain = prompt | model | output_parser
#         return chain.invoke({"question": question})

#     async def _arun(self, question: str):
#         return self._run(question)
    
# class GPT35TCodeGen(BaseTool):
#     name: str = "GPT35-Turbo_Code_Assistant"
#     description: str = "Use this tool, when the user wants code generated by GPT3.5 or GPT3.5-Turbo code assistant.\n" +\
#     "This tool accepts one input question:\n" + \
#     "[question]\n" + \
#     "Don't change the input question from the user and don't change answer from this tool\n."  +\
#     "Just pass it through to the user."
    
#     args_schema: Type[BaseModel] = AssistantInput

#     def _run(self, question: str):
#         prompt = ChatPromptTemplate.from_template(
#             "You are a code assistant. Answer questions in code with minimal to no explanation.\n" +
#             "Put brief one line comments on the code for explanation." +
#             "answer the following question: {question}")
#         model = ChatOpenAI(model="gpt-4-turbo-preview")
#         output_parser = StrOutputParser()

#         chain = prompt | model | output_parser
#         return chain.invoke({"question": question})

#     async def _arun(self, question: str):
#         return self._run(question)
    

# class GeneratorInput(BaseModel):
#     prompt: str = Field(description="The prompt for image generation.")

# class DalleImageGen(BaseTool):
#     name: str = "Dalle3_Image_Generator"
#     description: str = "Use this tool, when the user wants images generated by Dall-e-3.\n" +\
#     "This tool accepts one input prompt:\n" + \
#     "[prompt]\n" + \
#     "The output is a json blob with image path in it. Pass the output to the user."
    
#     args_schema: Type[BaseModel] = GeneratorInput
#     model_name: str = "dall-e-3"#,"dall-e-2"
#     image_folder: str = "lc_images"
#     image_number: int = 0

#     def _run(self, prompt: str):
#         client = OpenAI()
#         image_data = client.images.generate(
#                 model=self.model_name,
#                 prompt=prompt,
#                 size="1024x1024",
#                 quality="standard",
#                 n=1,
#         )
#         image = requests.get(image_data.data[0].url,stream=True)
#         if image.status_code == 200:
#             image_path = os.path.join(self.image_folder,
#                                       f"dall-e-3_{self.image_number}.png")
#             self.image_number += 1
#             # print("image_path:",image_path)
#             with open(image_path, 'wb') as f:
#                 for chunk in image:
#                     f.write(chunk)
#             return {"image_path":image_path}
#         return {}

#     async def _arun(self, prompt: str):
#         return self._run(prompt)
    

class RAGTool(BaseTool):
    name: str = "RAG_Assistant"
    description: str = (
        "Use this tool when the user wants to know anything related to LLM Powered Autonomous Agents.\n"
        "The questions could be about agent planning, memory, and tools.\n"
        "Examples and case studies of such agents and challenges are covered.\n"
        "This tool accepts one input question:\n"
        "[question]\n"
        "Don't change the input question from the user and don't change the answer from this tool.\n"
        "Just pass it through to the user."
    )

    retriever: Type[VectorStoreRetriever] = None
    llm: Type = None
    prompt: Type[ChatPromptTemplate] = None

    def __init__(self, llm, prompt):
        super().__init__()
        try:
            client = QdrantVectorStore.from_existing_collection(
                collection_name="my_documents",
                embedding=OpenAIEmbeddings(),
                url="http://localhost:6333"  # Qdrant server endpoint
            )
            self.retriever = client.as_retriever()
            self.llm = llm
            self.prompt = prompt
        except Exception as e:
            print("Error in loading RAG vector store:", e)

    args_schema: Type[BaseModel] = AssistantInput

    @staticmethod
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    def _run(self, question: str):
        rag_chain = (
            {"context": self.retriever | self.format_docs, "question": RunnablePassthrough()}
            | self.prompt
            | self.llm
            | StrOutputParser()
        )
        return rag_chain.invoke(question)

    async def _arun(self, question: str):
        return self._run(question)